# 밑바닥부터 시작하는 딥러닝

## 3장 신경망

### 1. 활성화 함수(Activation function)

#### 1-1. 계단 함수(Step function)

> 특정 지점에서 갑자기 값이 변하는 함수

```python
def step_function(x):
    return np.array(x > 0, dtype=np.int)
```



#### 1-2. 시그모이드 함수(Sigmoid)

> 신경망에서 자주 사용하는 활성화 함수

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```



#### 1-3. ReLU 함수(Rectified Linear Unit)

> 입력이 0을 넘으면 그 입력 그대로 출력, 0 이하이면 0을 출력하는 함수

```python
def relu(x):
    return np.maximum(0, x)
```



#### 1-4. 항등 함수(Identify funtion) - 회귀에 사용

> 입력과 출력이 항상 같은 함수



#### 1-5. 소프트맥스(Softmax function) - 분류에 사용

> 출력 총합이 1이 된다. (확률로 사용 가능)

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a

    return y
```





### 2. 손실 함수(Loss function)

#### 2-1. 오차제곱합(Sum of Squares for Error, SSE)

```python
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```



#### 2-2. 교차 엔트로피 오차(Cross Entropy Error, CEE)

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    delta = 1e-7  # np.log()에 0을 입력하면 음의 무한대가 되어 이를 방지
    return -np.sum(t * np.log(y + delta)) / batch_size
```

